{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVQE4FfdXtw2fNuMhxRKfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyotsnaEdathoot/Action-Recognition-Transformers-/blob/main/HAR_with_claude_30_12_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zJoJfLzkvUKs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torchvision import transforms\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "        )\n",
        "\n",
        "        # Positional embedding\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.proj(x)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "class ActionViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=101,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=mlp_ratio * embed_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        # MLP head\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch_size, channels, frames, height, width]\n",
        "        b, c, f, h, w = x.shape\n",
        "\n",
        "        # Reshape for 2D processing\n",
        "        x = rearrange(x, 'b c f h w -> (b f) c h w')\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Use CLS token for classification\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # Reshape back to include temporal dimension\n",
        "        x = rearrange(x, '(b f) d -> b f d', b=b)\n",
        "\n",
        "        # Global average pooling over frames\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n",
        "\n",
        "def train_step(model, optimizer, data, labels, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in val_loader:\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return total_loss / len(val_loader), 100. * correct / total\n",
        "\n",
        "# Data preprocessing\n",
        "def get_transforms(img_size=224):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(int(img_size * 1.14)),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps for better recognition:\n",
        "\n",
        "Data Preparation:\n",
        "Collect/use standard action datasets (UCF101, Kinetics, HMDB51)\n",
        "Implement video loading and preprocessing pipeline\n",
        "Apply temporal augmentations (random clips, frame skipping)\n"
      ],
      "metadata": {
        "id": "GwOLaHBLwehg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "class UCF101Dataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transform=None, frames_per_clip=16, frame_skip=2, split='train'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        self.frame_skip = frame_skip\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        self.videos = []\n",
        "        self.labels = []\n",
        "\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            for line in f:\n",
        "                video_path, label = line.strip().split()\n",
        "                self.videos.append(video_path)\n",
        "                self.labels.append(int(label))\n",
        "\n",
        "    def _load_video(self, video_path):\n",
        "        cap = cv2.VideoCapture(str(self.root_dir / video_path))\n",
        "        frames = []\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if self.split == 'train':\n",
        "            start_idx = np.random.randint(0, max(1, total_frames - self.frames_per_clip * self.frame_skip))\n",
        "        else:\n",
        "            start_idx = max(0, total_frames - self.frames_per_clip * self.frame_skip) // 2\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_idx)\n",
        "\n",
        "        for _ in range(self.frames_per_clip * self.frame_skip):\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "            if len(frames) >= self.frames_per_clip:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # If we don't have enough frames, duplicate the last frame\n",
        "        while len(frames) < self.frames_per_clip:\n",
        "            frames.append(frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "\n",
        "        return np.array(frames[::self.frame_skip])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.videos[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        video = self._load_video(video_path)\n",
        "\n",
        "        if self.transform:\n",
        "            frames = []\n",
        "            for frame in video:\n",
        "                frame = self.transform(frame)\n",
        "                frames.append(frame)\n",
        "            video = torch.stack(frames)\n",
        "\n",
        "        return video, label\n",
        "\n",
        "def get_data_loaders(root_dir, annotation_dir, batch_size=16, num_workers=4):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = UCF101Dataset(\n",
        "        root_dir=root_dir,\n",
        "        annotation_file=os.path.join(annotation_dir, 'trainlist01.txt'),\n",
        "        transform=train_transform,\n",
        "        split='train'\n",
        "    )\n",
        "\n",
        "    val_dataset = UCF101Dataset(\n",
        "        root_dir=root_dir,\n",
        "        annotation_file=os.path.join(annotation_dir, 'testlist01.txt'),\n",
        "        transform=val_transform,\n",
        "        split='val'\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "qyZK_LDRw8bJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Improvements:\n",
        "Add temporal attention layers\n",
        "Implement 3D patch embeddings\n",
        "Use pretrained weights from ImageNet\n",
        "\n",
        "Training Enhancements:\n",
        "Implement learning rate scheduling\n",
        "Add model checkpointing\n",
        "Use mixed precision training\n",
        "Add validation metrics (accuracy, confusion matrix)"
      ],
      "metadata": {
        "id": "YlpMRICwzce3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange, repeat\n",
        "import timm\n",
        "\n",
        "class Space3DPatchEmbedding(nn.Module):\n",
        "    def __init__(self, video_size=(16, 224, 224), patch_size=(2, 16, 16), in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.t, self.h, self.w = video_size\n",
        "        self.pt, self.ph, self.pw = patch_size\n",
        "        self.n_patches = (self.t // self.pt) * (self.h // self.ph) * (self.w // self.pw)\n",
        "        self.projection = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.projection(x)\n",
        "        x = rearrange(x, 'b e t h w -> b (t h w) e')\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=B)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.position_embeddings\n",
        "        return x\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = rearrange(q, 'b h (t n) d -> b h t n d', t=T)\n",
        "        k = rearrange(k, 'b h (t n) d -> b h t n d', t=T)\n",
        "        v = rearrange(v, 'b h (t n) d -> b h t n d', t=T)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v)\n",
        "        x = rearrange(x, 'b h t n d -> b (t n) (h d)')\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class ImprovedActionViT(nn.Module):\n",
        "    def __init__(self, video_size=(16, 224, 224), patch_size=(2, 16, 16), in_channels=3,\n",
        "                 num_classes=101, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = Space3DPatchEmbedding(\n",
        "            video_size=video_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        self.temporal_attn = nn.ModuleList([\n",
        "            TemporalAttention(embed_dim, num_heads=num_heads)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.transformer = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=mlp_ratio * embed_dim,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        T = x.size(1) // (16 * 16)\n",
        "\n",
        "        for temporal_layer, transformer_layer in zip(self.temporal_attn, self.transformer):\n",
        "            x = x + temporal_layer(x, T)\n",
        "            x = transformer_layer(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def load_imagenet_weights(model):\n",
        "    vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "    # Load patch embedding weights\n",
        "    with torch.no_grad():\n",
        "        w = vit.patch_embed.proj.weight.data\n",
        "        model.patch_embed.projection.weight.data[:, :, 1, :, :].copy_(w)\n",
        "\n",
        "        # Load transformer weights\n",
        "        for i, layer in enumerate(model.transformer):\n",
        "            # Copy attention weights\n",
        "            vit_layer = vit.blocks[i].attn\n",
        "            layer.self_attn.in_proj_weight.data.copy_(vit_layer.qkv.weight.data)\n",
        "            layer.self_attn.in_proj_bias.data.copy_(vit_layer.qkv.bias.data)\n",
        "            layer.self_attn.out_proj.weight.data.copy_(vit_layer.proj.weight.data)\n",
        "            layer.self_attn.out_proj.bias.data.copy_(vit_layer.proj.bias.data)\n",
        "\n",
        "            # Copy MLP weights\n",
        "            vit_mlp = vit.blocks[i].mlp\n",
        "            layer.linear1.weight.data.copy_(vit_mlp.fc1.weight.data)\n",
        "            layer.linear1.bias.data.copy_(vit_mlp.fc1.bias.data)\n",
        "            layer.linear2.weight.data.copy_(vit_mlp.fc2.weight.data)\n",
        "            layer.linear2.bias.data.copy_(vit_mlp.fc2.bias.data)\n",
        "\n",
        "        # Load normalization weights\n",
        "        model.norm.weight.data.copy_(vit.norm.weight.data)\n",
        "        model.norm.bias.data.copy_(vit.norm.bias.data)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_model(num_classes=101, pretrained=True):\n",
        "    model = ImprovedActionViT(num_classes=num_classes)\n",
        "    if pretrained:\n",
        "        model = load_imagenet_weights(model)\n",
        "    return model"
      ],
      "metadata": {
        "id": "WpINnEwGyFrl"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}